#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
RAG Integration for ShowupSquared Content Generator.

Connects the vector database, cache manager, and tokenizer with the
existing Claude API workflow to reduce token usage and improve relevance.
"""

import os
import re
import asyncio
import hashlib
import logging
from typing import Dict, Any, Optional

# Import our RAG components
from .token_counter import count_tokens
from .cache_manager import cache
from .textbook_vector_db import get_vector_db

# Configure logging
logger = logging.getLogger(__name__)


async def generate_with_claude_rag(prompt: str, 
                                   system_prompt: str = "", 
                                   max_tokens: int = 4000, 
                                   temperature: float = 0.7, 
                                   model: str = "claude-3-5-sonnet",
                                   handbook_path: Optional[str] = None,
                                   query: Optional[str] = None):
    """Generate content using Claude API with RAG integration
    
    Args:
        prompt: The prompt to send to Claude
        system_prompt: Optional system prompt 
        max_tokens: Maximum tokens to generate
        temperature: Temperature for text generation
        model: Claude model to use
        handbook_path: Path to the textbook/handbook if available
        query: Optional query to extract relevant content from handbook
        
    Returns:
        Generated text from Claude
    """
    try:
        # Import the actual Claude API client
        from showup_core.api_client import generate_with_claude as actual_generate
    except ImportError:
        # Mock function for testing without the API client
        logger.warning("Claude API client not found. Using mock response.")
        
        async def actual_generate(**kwargs):
            prompt_tokens = count_tokens(kwargs.get('prompt', ''))
            system_tokens = count_tokens(kwargs.get('system_prompt', ''))
            logger.info(f"Would send {prompt_tokens} prompt tokens and {system_tokens} system tokens to Claude")
            return f"[This is a mock response that would be generated by Claude {kwargs.get('model')}]"
    
    # Track the original prompt length
    original_prompt_len = len(prompt)
    original_token_count = count_tokens(prompt)
    
    # If handbook path is provided, use RAG to extract relevant content
    if handbook_path and os.path.exists(handbook_path):
        # Create a textbook ID based on the path
        textbook_id = hashlib.md5(handbook_path.encode()).hexdigest()

        # SAFEGUARD 2: Ensure we always have some kind of query
        if not query:
            query = "general overview"
            logger.warning(
                f"No query provided for {textbook_id}, using fallback query"
            )
        
        # Generate cache key for retrieval results
        cache_key = cache.get_cache_key('retrieval', {
            'textbook_id': textbook_id,
            'query': query,
        })
        
        # Try to get from cache first
        cached_chunks = cache.get(cache_key, max_age=86400*7)  # 1 week cache
        
        if cached_chunks:
            logger.info(f"Using cached retrieval results for {textbook_id}")
            relevant_chunks = cached_chunks
        else:
            # If no cached results, need to index and retrieve
            try:
                # Read the handbook content
                with open(handbook_path, 'r', encoding='utf-8') as f:
                    handbook_content = f.read()

                # Lazily obtain vector database instance
                db = get_vector_db()

                # Index the textbook if needed (will use cached index if unchanged)
                await db.index_textbook_async(handbook_content, textbook_id)

                # Query for relevant content
                results = await db.query_textbook_async(textbook_id, query, top_k=3)
                
                # Extract just the content from results
                relevant_chunks = [r['content'] for r in results]
                
                # Cache the results for future use
                cache.set(cache_key, relevant_chunks)
                
                logger.info(f"Retrieved {len(relevant_chunks)} relevant chunks from {textbook_id}")
            except Exception as e:
                logger.exception(f"Error retrieving content: {e}")
                # Continue with original prompt if retrieval fails
                relevant_chunks = []
        
        # If we found relevant chunks, use them. If not, provide a minimal placeholder.
        if relevant_chunks:
            # Construct a modified prompt with relevant chunks
            handbook_content = "\n\n".join(relevant_chunks)
            
            # Add relevant content to the prompt in a clear format
            rag_prefix = (
                "### RELEVANT CONTEXT FROM HANDBOOK:\n\n" + 
                handbook_content + 
                "\n\n### ORIGINAL PROMPT:\n\n"
            )
        else:
            # Provide a placeholder when no chunks are found to avoid using the full handbook
            logger.warning(f"No relevant chunks found for query: {query[:100]}...")
            rag_prefix = "### NOTE: No directly relevant content found in handbook for this query.\n\n### ORIGINAL PROMPT:\n\n"
            
        # Create the enhanced prompt
        enhanced_prompt = rag_prefix + prompt
            
        # Calculate token savings
        try:
            with open(handbook_path, 'r', encoding='utf-8') as f:
                full_content = f.read()
                full_token_count = count_tokens(full_content)
                enhanced_token_count = count_tokens(enhanced_prompt)
                
                token_savings = full_token_count - enhanced_token_count + original_token_count
                savings_percent = (token_savings / full_token_count) * 100
                
                logger.info(f"Token savings: {token_savings:,} tokens (approximately {savings_percent:.1f}%)")
        except Exception as e:
            logger.error(f"Error calculating token savings: {e}")
        
        # Use the enhanced prompt
        prompt = enhanced_prompt
    
    # SAFEGUARD 4: Add token count guard rail
    token_count = count_tokens(prompt)
    logger.info(f"Prompt size: {token_count} tokens")
    
    # Log the first part of the prompt for debugging
    logger.debug(f"First 300 chars of prompt: {prompt[:300]!r}")
    
    # Implement token guard rail to prevent massive API calls
    if token_count > 5000:
        error_msg = f"Refusing to send {token_count} tokens - RAG likely failed, would exceed safe limit of 5000"
        logger.error(error_msg)
        return f"ERROR: {error_msg}\n\nPlease check the logs and fix the RAG system configuration."
    
    # Generate with Claude
    logger.info(f"Generating with {model}, max_tokens={max_tokens}, temp={temperature}")
    return await actual_generate(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        model=model
    )


async def enhanced_generate_content(variables: Dict[str, Any], template: str, settings: Optional[Dict[str, Any]] = None) -> str:
    """Enhanced content generation using vector DB and caching
    
    Args:
        variables: Variables for template substitution
        template: Template string
        settings: Generation settings
        
    Returns:
        Generated content
    """
    if settings is None:
        settings = {}
    
    # SAFEGUARD 1: Remove any pre-populated handbook text
    # This ensures we never accidentally use a full handbook that was pre-loaded
    variables.pop("handbook_text", None)
    
    # Get handbook path if provided
    handbook_path = variables.get('handbook_path', '')
    
    # Generate cache key for this request - omit volatile fields like word_count
    cache_key = cache.get_cache_key('content_generation', {
        'step_title': variables.get('step_title', ''),
        'content_outline': variables.get('content_outline', ''),
        'model': variables.get('model', 'claude-3-5-sonnet')
        # Removed word_count to improve cache hits
    })
    
    # Try to get from cache first
    cached_content = cache.get(cache_key)
    if cached_content:
        logger.info(f"Cache hit for {variables.get('step_title', 'unknown')}")
        return cached_content
    
    logger.info(f"Cache miss for {variables.get('step_title', 'unknown')}, generating content")
    
    # Process template substitution
    try:
        # Convert all variables to strings
        str_variables = {k: str(v) if not isinstance(v, str) else v 
                        for k, v in variables.items()}
        
        # Try to use the safer format method first
        try:
            # Replace {{var}} with {var} for python's format
            formatted_template = re.sub(r'\{\{(\w+)\}\}', r'{\1}', template)
            prompt_text = formatted_template.format(**str_variables)
        except (KeyError, ValueError):
            # Fall back to manual replacement if format fails
            prompt_text = template
            for key, value in str_variables.items():
                prompt_text = prompt_text.replace(f"{{{{{key}}}}}", value)
    except Exception:
        logger.exception("Error during template substitution")
        # Fall back to a simpler approach in case of error
        prompt_text = template + "\n\nContext:\n" + str(variables)
    
    # Query to use with RAG
    query = variables.get('step_title', '') + " " + variables.get('content_outline', '')
    
    # Call Claude API with RAG integration
    content = await generate_with_claude_rag(
        prompt=prompt_text,
        system_prompt=settings.get('system_prompt', ''),
        max_tokens=settings.get('max_tokens', 4000),
        temperature=settings.get('temperature', 0.7),
        model=variables.get('model', 'claude-3-5-sonnet'),
        handbook_path=handbook_path,
        query=query
    )
    
    # Cache the result
    cache.set(cache_key, content)
    
    return content


async def main():
    """Test the RAG pipeline with a sample handbook"""
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    
    handbook_path = "C:\\Users\\User\\Desktop\\ShowupSquaredV4 (2)\\ShowupSquaredV4\\ShowupSquaredV4\\showup-tools\\simplified_app\\EHS Student Catalog_Handbook from Canva.md"
    
    if not os.path.exists(handbook_path):
        logger.error(f"Handbook not found: {handbook_path}")
        return
    
    # Test variables
    variables = {
        'step_title': 'Academic Integrity Policy',
        'content_outline': 'Explain the academic integrity policy, including what constitutes academic dishonesty and the consequences.',
        'handbook_path': handbook_path,
        'model': 'claude-3-5-sonnet',
        'word_count': 400
    }
    
    # Simple template
    template = """
    Please create educational content about {{step_title}} based on the following outline:
    
    {{content_outline}}
    
    The content should be approximately {{word_count}} words.
    """
    
    # Generate content with RAG
    logger.info("Testing RAG-enhanced content generation...")
    content = await enhanced_generate_content(variables, template)
    
    # Print the result
    print("\n======= GENERATED CONTENT =======\n")
    print(content)
    print("\n================================\n")


if __name__ == "__main__":
    # Run the test
    asyncio.run(main())
